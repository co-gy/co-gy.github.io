# 贝尔曼公式

# 概念

贝尔曼公式描述的是当前时刻与下一时刻的state-value的关系

return: 一条trajectory的discount reward

state value: 当前state出发，discount reward的期望

ps: 在deterministic的模型中，return就是state-value，但是对于stochastic的模型，return是一次采样，state value是表示从当前state出发能够获得的reward的和的期望

# 符号

policy: $$\pi(a|s)$$

probability distribution: $$p(s'|s, a)$$，$$p(r|s, a)$$

discount rate: $$\gamma \in (0, 1)$$

return: $$G_t = R_{t+1}+\gamma R_{t+2}+ \gamma^2 R_{t+3} +\dots$$

state value: $$v_{\pi}(s) = \mathbb{E}[G_t | S_t=s]$$

贝尔曼公式：

$$
v_{\pi}(s)=\sum_a \pi(a|s) \bigg[\sum_r p(r|s, a)r + \gamma \sum_{s'} p(s'|s, a)v_{\pi}(s') \bigg]
$$

# 推导

目的：我们希望能够使用MDP中的policy和model(概率分布)来表示state value

根据state value的定义式，能够拆成两份：

 

$$
v_{\pi}(s) = \mathbb{E}[G_t | S_t=s] = \mathbb{E}[R_{t+1}+\gamma G_{t+1} | S_t=s] = \underbrace{\mathbb{E}[R_{t+1}|S_t=s]}_1+\gamma \underbrace{\mathbb{E}[G_{t+1}|S_t=s]}_2
$$

第一部分：表示的是在当前state立即获得的reward

$$
\mathbb{E}[R_{t+1}|S_t=s] = \sum_a \pi(a|s) \mathbb{E}[R_{t+1}|S_t=s, A_t=a] = \sum_a \pi(a|s) \sum_r p(r|s, a)r
$$

第二部分：表示的是在当前state，下一个获得的discount return的期望

$$
\mathbb{E}[G_{t+1}|S_t=s] = \sum_{s'} p(s'|s) \mathbb{E}[G_{t+1}|S_t=s, S_{t+1}=s']
$$

根据马尔可夫性质，下一刻仅与当前一刻有关。$G_{t+1}$是在$t+1$时刻状态为$s'$出发所得的discount return。所以$G_{t+1}$与$t$时刻$s$无关。于是式子简化为：

$$
\mathbb{E}[G_{t+1}|S_t=s] = \sum_{s'} p(s'|s) \mathbb{E}[G_{t+1}|S_{t+1}=s'] = \sum_{s'}v_{\pi}(s') \sum_a  \pi (a|s)p(s'|s,a)
$$

将两部分相加，得到结果

# 矩阵-向量形式

# 概率论计算技巧

以上推算用了一些概率计算技巧，总结如下